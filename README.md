# Navigation Aid System for Blind People

## Overview
This project aims to develop a sophisticated navigation aid system for blind individuals using modern tools of visual prompting, conversational text, and speech recognition. Leveraging the capabilities of platforms like ChatGPT-4, this system intends to provide comprehensive information about the surroundings, enabling blind people to navigate more independently and safely.

## Background
Traditional navigation aids have limitations that can be overcome with advanced technology. By integrating visual prompting (Segment Anything Model - SAM), conversational AI (ChatGPT, Llama), and speech recognition, we can create a more informative and helpful system for the visually impaired.

## Objective
The goal is to enhance the existing standard navigation tools with modern mechanisms, making use of GPT-4's ability to analyze image inputs and integrate speech for a seamless user experience.

## System Components
- **Visual Prompting**: Utilizes the camera to capture images and extract information.
- **Conversational AI**: Generates descriptions of the surroundings.
- **Speech Recognition**: Allows users to interact with the system using voice commands.

## User Interaction
1. Capture images of the surroundings with the camera.
2. Extract information using visual prompting.
3. Generate a description of the surroundings.
4. Use text-to-speech to provide auditory feedback to the user.
5. Allow voice interactions for questions and commands.